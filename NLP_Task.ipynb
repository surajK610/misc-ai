{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLtSZyeOUbg1WkPFgHXjU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajK610/AI/blob/master/NLP_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "vader.polarity_scores('happy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5nuCBwl9YXl",
        "outputId": "c0f5d05b-85c2-40cc-a184-614fa479931c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "# from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "num_documents = 5000 # INFO: Feel free to change this to load in less documents for debugging but otherwise keep it at 5000 to train the Topic Model\n",
        "FILEPATH = \"/content/drive/MyDrive/CSCI 1460 - Computational Linguistics/Homework/A2 - Topic Modeling/\" # TODO: Update this to the filepath of your copy of the assignment, e.g. /content/drive/MyDrive/Topic Modeling/\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc_locations = []\n",
        "spacy_processed_docs = []\n",
        "\n",
        "if exists(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\"):\n",
        "  with open(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\", 'rb') as f:\n",
        "    spacy_processed_docs, doc_locations = pickle.load(f)\n",
        "    f.close()\n",
        "else:\n",
        "  with open(f'{FILEPATH}articles_sampled_data.csv', 'r', encoding='utf-8') as f:\n",
        "    for i, row in tqdm(enumerate(csv.DictReader(f, delimiter=','))):\n",
        "      if i == num_documents:\n",
        "        break\n",
        "      if i % 500 == 0:\n",
        "        print(\"Processing row %d\"%i)\n",
        "      try:\n",
        "        parsed = nlp(row[\"content\"])\n",
        "        source_name = row[\"location\"]\n",
        "      except ValueError:\n",
        "        continue\n",
        "      spacy_processed_docs.append(parsed)\n",
        "      doc_locations.append(source_name)\n",
        "    f.close()\n",
        "\n",
        "  with open(f\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\", 'wb') as f:\n",
        "    pickle.dump((spacy_processed_docs, doc_locations), f)\n",
        "    f.close()\n",
        "\n",
        "def preprocess(raw_X: List[str]) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Performs splitting on whitespace on all raw strings in a list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw_X : List[str]\n",
        "        A list of raw strings (tweets)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[List[str]]\n",
        "        A list of preprocessed tweets (which are now lists of words)\n",
        "    \"\"\"\n",
        "    # TODO Basic tokenization just based on whitespace, with no other preprocessing\n",
        "    return [x.split() for x in raw_X]\n",
        "\n",
        "def preprocess_part2(parsed_tweets: List[spacy_doc]) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Preprocesses the spacy-parsed tweets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parsed_tweets : List[spacy_doc]\n",
        "        A list of tweets parsed by spacy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        A list of preprocessed tweets formatted as lists of tokens (lists of strings)\n",
        "    \"\"\"\n",
        "    preproc = []\n",
        "    for r in parsed_tweets:\n",
        "        words = []\n",
        "        for word in r:\n",
        "            if not (word.is_stop or word.is_punct or word.is_space):\n",
        "                if word.pos_ == \"NUM\" or word.like_num:\n",
        "                    words.append(\"<NUM>\")\n",
        "                else:\n",
        "                    words.append(word.lemma_.lower().strip())\n",
        "        preproc.append(words)\n",
        "\n",
        "    counts = {}\n",
        "    for r in preproc:\n",
        "        for word in r:\n",
        "            counts[word] = counts.get(word, 0) + 1\n",
        "    K = 1000\n",
        "    topk = [w[0] for w in sorted(counts.items(), key=lambda x: x[1], reverse=True)[:K]]\n",
        "\n",
        "    return [[w if w in topk else \"<OOV>\" for w in r] for r in preproc]\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "class BOW_Classifier:\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    clf : LogisticRegression\n",
        "        A logistic regression classifier\n",
        "    dv : DictVectorizer\n",
        "        A dictionary vectorizer for turning dictionaries into matrices\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.clf = LogisticRegression(max_iter=150)\n",
        "        self.dv = None  # You are allowed to not use this attribute as well as add more to the init method if you wish\n",
        "\n",
        "    def featurize(\n",
        "        self, preproc_X: np.ndarray[List[str]], is_test: bool = False\n",
        "    ) -> csr_matrix:\n",
        "        \"\"\"\n",
        "        Turns a list of preprocessed tweets into a binary bag of words\n",
        "        matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        preproc_X : np.ndarray[List[str]]\n",
        "            A list of preprocessed tweets\n",
        "        is_test: bool, default=False\n",
        "            Whether featurization should be done using features learned during training (is_test=True)\n",
        "            or whether it should be done with features extracted from scratch using preproc_X (is_test=False)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        csr_matrix\n",
        "            A matrix with rows corresponding to tweets and columns corresponding to words\n",
        "        \"\"\"\n",
        "        dicts = [{w: 1 for w in x} for x in preproc_X]\n",
        "\n",
        "        if is_test:\n",
        "            return self.dv.transform(dicts)\n",
        "        else:\n",
        "            self.dv = DictVectorizer()\n",
        "            X = self.dv.fit_transform(dicts)\n",
        "        return X\n",
        "\n",
        "    def train(self, X_train: np.ndarray[List[str]], y_train: np.ndarray[int]):\n",
        "\n",
        "        X_train_feat = self.featurize(X_train)\n",
        "        self.clf.fit(X_train_feat, y_train)\n",
        "\n",
        "    def test(self, X_test: np.ndarray[List[str]]) -> np.ndarray[int]:\n",
        "\n",
        "        X_test_feat = self.featurize(X_test, is_test=True)\n",
        "        return self.clf.predict(X_test_feat)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def run_kfold_crossval(\n",
        "    model: BOW_Classifier, X: List[List[str]], y: List[int], k: int = 5\n",
        ") -> List[float]:\n",
        "    accs = []\n",
        "\n",
        "    X, y = np.array(X, dtype=list), np.array(y)\n",
        "    skf = StratifiedKFold(n_splits=k)\n",
        "\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        X_train, X_test, y_train, y_test = (\n",
        "            X[train_idx],\n",
        "            X[test_idx],\n",
        "            y[train_idx],\n",
        "            y[test_idx],\n",
        "        )\n",
        "        model.train(X_train, y_train)\n",
        "\n",
        "        y_pred = model.test(X_test)\n",
        "        accs.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "    return accs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "d1kEDgZz9gHA",
        "outputId": "fd2911b3-0e1b-4614-e1b3-1d03e55c42f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-26 12:30:14.054447: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-26 12:30:14.054522: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-26 12:30:14.054575: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-10-26 12:30:18.537060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aab25f0e77fe>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mspacy_processed_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{FILEPATH}spacy_processed_docs_{num_documents}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mspacy_processed_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exists' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def binary_term_doc_matrix(docs : List[spacy.tokens.Doc]) -> Tuple[np.ndarray[np.float64], Dict[int, str]]:\n",
        "  c = Counter()\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "\n",
        "  for doc in docs:\n",
        "    c.update([token.lemma_.lower() for token in doc])\n",
        "\n",
        "  most_common = [word for word, _ in c.most_common()]\n",
        "  for rank in range(len(most_common)):\n",
        "    word = most_common[rank]\n",
        "    word2idx[word] = rank\n",
        "    idx2word[rank] = word\n",
        "\n",
        "  M = np.zeros((len(docs), len(most_common)))\n",
        "  for i in range(len(docs)):\n",
        "    for token in docs[i]:\n",
        "      M[i, word2idx[token.lemma_.lower()]] = 1\n",
        "\n",
        "  return M, idx2word\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def train_topic_model(term_doc_mat : np.ndarray[np.float64], n_topics : int = 10, random_state = 42) -> LatentDirichletAllocation:\n",
        "  lda = LatentDirichletAllocation(n_components=n_topics, random_state = random_state)\n",
        "  lda.fit(term_doc_mat)\n",
        "  return lda\n",
        "\n",
        "from collections import Counter # HINT: you may find this useful\n",
        "import math\n",
        "\n",
        "def preprocess_doc(doc : spacy.tokens.Doc) -> List[str]:\n",
        "  proc_doc = []\n",
        "  for token in doc:\n",
        "    if not (token.is_stop or token.is_punct or token.pos_=='SPACE'):\n",
        "      proc_doc.append(token.lemma_.lower())\n",
        "  return proc_doc\n",
        "\n",
        "def create_vocab(proc_docs : List[List[str]], vocab_cutoff : int = 5000) -> List[str]:\n",
        "  vocab = Counter()\n",
        "  for proc_doc in proc_docs:\n",
        "    vocab.update(proc_doc)\n",
        "\n",
        "  vocab = vocab.most_common(vocab_cutoff)\n",
        "  vocab = [word[0] for word in vocab]\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "dRx_MBmO9llt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxU2RtLo9Rzr"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import torch, codecs, random\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import datasets\n",
        "from datasets import load_metric\n",
        "from google.colab import output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from transformers import PreTrainedTokenizer\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  A PyTorch Dataset for our tweets that can be iterated through using __getitem__\n",
        "  \"\"\"\n",
        "  def __init__(self, tweets : List[str], sentiments : List[int], tokenizer : PreTrainedTokenizer) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the TweetDataset from a list of tweets, their corresponding sentiments, and a tokenizer.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    tweets : List[str]\n",
        "      A list of tweets, where each tweet is a string\n",
        "    sentiments: List[int]\n",
        "      A list of sentiments represented as integers ('negative': 0, 'neutral': 1, 'positive': 2)\n",
        "    tokenizer : PreTrainedTokenizer\n",
        "      Any PreTrainedTokenizer from HuggingFace can be used to encode the string inputs for a model\n",
        "    \"\"\"\n",
        "    self.tweets = tweets\n",
        "    self.sentiments = sentiments\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = tokenizer.model_max_length\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the number of tweets in the dataset.\n",
        "    \"\"\"\n",
        "    return len(self.tweets)\n",
        "\n",
        "  def __getitem__(self, index : int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Retrieve a preprocessed data item from the dataset at the specified index.\n",
        "    This is called when iterating through a TweetDataset\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    index : int\n",
        "        The index of the data item to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    Dict[str, Any]\n",
        "        A dictionary containing the preprocessed data for the given index.\n",
        "        The dictionary includes the following keys:\n",
        "        - 'input_ids': Encoded input IDs for the tweet.\n",
        "        - 'attention_mask': Attention mask for the tweet.\n",
        "        - 'labels': Sentiment label as a PyTorch tensor.\n",
        "    \"\"\"\n",
        "    tweet = str(self.tweets[index])\n",
        "    sentiments = self.sentiments[index]\n",
        "\n",
        "    encoded_tweet = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens      = True,\n",
        "      max_length              = self.max_len,\n",
        "      return_token_type_ids   = False,\n",
        "      return_attention_mask   = True,\n",
        "      return_tensors          = \"pt\",\n",
        "      padding                 = \"max_length\",\n",
        "      truncation              = True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'input_ids': encoded_tweet['input_ids'][0],\n",
        "      'attention_mask': encoded_tweet['attention_mask'][0],\n",
        "      'labels': torch.tensor(sentiments, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# use this for the tokenizer argument of the TweetDataset\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# define the following TweetDataset objects... be careful to split the data as previously specified\n",
        "train_dataset = TweetDataset(tweets = trainset, sentiments = trainlabs, tokenizer = tokenizer)\n",
        "validation_dataset = TweetDataset(tweets = valset, sentiments = vallabs, tokenizer = tokenizer)\n",
        "test_dataset = TweetDataset(tweets = testset, sentiments = testlabs, tokenizer = tokenizer)\n",
        "\n",
        "# now construct DataLoader objects from the TweetDataset objects\n",
        "# remember that the TweetDataset class is a child class of torch.utils.data.Dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "def update_metrics(metrics: List[datasets.Metric], predictions: torch.Tensor, labels: torch.Tensor) -> None:\n",
        "  \"\"\"\n",
        "  Update a list of metrics with new predictions and labels\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  metrics : List[Metric]\n",
        "      List of metrics.\n",
        "  predictions : torch.Tensor\n",
        "      Tensor of predictions of shape (batch_size, ...)\n",
        "  labels : torch.Tensor\n",
        "      Tensor of labels of shape (batch_size, ...)\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  None\n",
        "  \"\"\"\n",
        "  # Nothing TODO here! This updates metrics based on a batch of predictions\n",
        "  # and a batch of labels.\n",
        "  for metric in metrics:\n",
        "    metric.add_batch(predictions=predictions, references=labels)\n",
        "\n",
        "def evaluate(model: torch.nn.Module, test_dataloader: torch.utils.data.DataLoader,\n",
        "             device: torch.device, metric_strs: List[str]) -> Dict[str, float]:\n",
        "  \"\"\"\n",
        "  Evaluate a PyTorch Model\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  model : torch.nn.Module\n",
        "      The model to be evaluated.\n",
        "  test_dataloader : torch.utils.data.DataLoader\n",
        "      DataLoader containing testing examples.\n",
        "  device : torch.device\n",
        "      The device that the evaluation will be performed on.\n",
        "  metric_strs : List[str]\n",
        "      The names of Hugging Face metrics to use.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  Dict[str, float]\n",
        "      Dictionary of metric names mapped to their values.\n",
        "  \"\"\"\n",
        "  # load metrics\n",
        "  metrics = [load_metric(x) for x in metric_strs] # could add more here!\n",
        "  model.eval()\n",
        "\n",
        "  # we like progress bars :)\n",
        "  progress_bar = tqdm(range(len(test_dataloader)))\n",
        "  # HINT: progress_bar.update(1) should be used to show progress after an iteration\n",
        "\n",
        "  # TODO: Fill in the evaluate function by applying the model with the dataloader\n",
        "\n",
        "  for batch in test_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # deactivate the autograd engine, which saves memory and will speed this up\n",
        "    with torch.no_grad():\n",
        "      # perform forward pass\n",
        "      outputs = model(**batch)\n",
        "\n",
        "    # convert logits to the predictions\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Update the metrics\n",
        "    update_metrics(metrics, predictions, batch[\"labels\"])\n",
        "\n",
        "    progress_bar.update(1)\n",
        "\n",
        "  # compute and return metrics\n",
        "  computed = {}\n",
        "  for m in metrics:\n",
        "    computed = {**computed, **m.compute()}\n",
        "\n",
        "  return computed\n",
        "\n",
        "def train(model: torch.nn.Module, optimizer: Optimizer, num_epochs: int,\n",
        "          train_dataloader: DataLoader, validation_dataloader: DataLoader,\n",
        "          lr_scheduler: Any, device: torch.device) -> None:\n",
        "  for epoch in range(num_epochs):\n",
        "    # put the model in training mode (important that this is done each epoch,\n",
        "    # since we put the model into eval mode during validation)\n",
        "    model.train()\n",
        "\n",
        "    # load metrics\n",
        "    metrics = [load_metric(x) for x in [\"accuracy\"]] # could add more here!\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} training:\")\n",
        "    progress_bar = tqdm(range(len(train_dataloader)))\n",
        "\n",
        "    # TODO: Fill in the rest of the train function by applying the model with the dataloader\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(**batch)\n",
        "\n",
        "      # give predictions and labels to metrics\n",
        "      # requires that we convert logits to predictions\n",
        "      logits = outputs.logits\n",
        "      predictions = torch.argmax(logits, dim=-1)\n",
        "      update_metrics(metrics, predictions, batch[\"labels\"])\n",
        "\n",
        "      # backpropagation\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters with optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # update learning rate\n",
        "      lr_scheduler.step()\n",
        "\n",
        "      # clear gradients\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar.update(1)\n",
        "    # print the epoch's average metrics\n",
        "    print(f\"Epoch {epoch+1} average training metrics: accuracy={metrics[0].compute()['accuracy']}\")\n",
        "\n",
        "    # normally, validation would be more useful when training for many epochs\n",
        "    print(\"Running validation:\")\n",
        "    # TODO: evaluate model on validation dataset\n",
        "    val_metrics = evaluate(model, validation_dataloader, device, ['accuracy'])\n",
        "    print(f\"Epoch {epoch+1} validation: accuracy={val_metrics['accuracy']}\")\n",
        "\n",
        "pretrained_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n"
      ]
    }
  ]
}